{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0e44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, when, udf, count, isnan, desc\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, QuantileDiscretizer, VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from typing import Any, List, Tuple\n",
    "from pyspark.ml import Transformer\n",
    "ud = udf(lambda row: unidecode(row))\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b6ce4ba",
   "metadata": {},
   "source": [
    "### Data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea9edf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.executor.memory\", \"70g\")\n",
    "        .config(\"spark.driver.memory\", \"50g\")\n",
    "        .config(\"spark.sql.analyzer.failAmbiguousSelfJoin\", False)\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "        .config(\"spark.memory.offHeap.size\", \"10g\")\n",
    "        .config(\"spark.default.parallelism\", 1)\n",
    "        .config(\"spark.sql.shuffle.partitions\", 1)\n",
    "        # .config(\"spark.driver.maxResultSize\", \"4g\") # Increase to higher value\n",
    "        # .config(\"spark.driver.warn.largeBroadcast\", \"false\") # Remove warnings\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7c29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_files_to_dataframe(directory, file_prefix, schema_csv=None):\n",
    "    all_training_files = [file for file in os.listdir(directory) if file.startswith(file_prefix)]\n",
    "    if schema_csv:\n",
    "        return (\n",
    "            spark.read.options(header=\"True\", inferSchema=\"False\").schema(schema_csv).csv(all_training_files)\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            spark.read.options(header=\"True\", inferSchema=\"False\").csv(all_training_files)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ad74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_director_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        dirData = json.load(f)\n",
    "    movies = dirData['movie']\n",
    "    dirs = dirData['director']\n",
    "    directors = {\n",
    "        value: dirs[key] if key in dirs.keys() else 'NULL'\n",
    "        for key, value in movies.items()\n",
    "    }\n",
    "    df_directors = pd.DataFrame({'tconst': directors.keys(), 'directors': directors.values()})\n",
    "    return spark.createDataFrame(df_directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721cb86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_writer_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        writerData = json.load(f)\n",
    "    writers = []\n",
    "    movies = []\n",
    "    for i in writerData:\n",
    "        writers.append(i['writer'])\n",
    "        movies.append(i['movie'])\n",
    "    df_writers = pd.DataFrame({'tconst': movies, 'writer': writers})\n",
    "    return spark.createDataFrame(df_writers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e328f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_director_data(df, directors):\n",
    "    df = df.join(directors, on='tconst')\n",
    "    directorCount = directors.groupBy('directors').count()\n",
    "    avgVotesDirector = df.groupBy('directors').avg('numVotes')\n",
    "    df = df.join(directorCount, on='directors')\n",
    "    df = df.join(avgVotesDirector, on='directors')\n",
    "    df = df.withColumnRenamed('count', 'moviesByDirector')\n",
    "    df = df.withColumnRenamed('avg(numVotes)', 'avgVotesDirector')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a95238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_writer_data(df, writers):\n",
    "    numWriters = writers.groupBy('tconst').count()\n",
    "    df = df.join(numWriters, on='tconst')\n",
    "    df = df.withColumnRenamed('count', 'numWriters')\n",
    "    written = writers.groupBy('writer').count()\n",
    "    counts = writers.join(written, on='writer')\n",
    "    counts = counts.groupBy('tconst').sum('count')\n",
    "    df = df.join(counts, on='tconst')\n",
    "    df = df.withColumnRenamed('sum(count)', 'totalMoviesWritten')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbea6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_columns(input_df_to_clean, primary_title_col, original_title_col, start_year_col, end_year_col, runtime_minutes_col, num_votes_col, label_col=None):\n",
    "    cleaned_df = (\n",
    "        input_df_to_clean\n",
    "        .drop(\"\")\n",
    "        .withColumn('numVotesIsNull', col(\"numVotes\").isNull().cast(T.IntegerType()))\n",
    "        .withColumn(\"runtimeMinutesIsNull\", when(col(runtime_minutes_col).isNull() | (col(runtime_minutes_col) == \"\\\\N\"), 1).otherwise(0))\n",
    "        .withColumn(primary_title_col, when(col(primary_title_col).isNull(), col(original_title_col)).otherwise(col(primary_title_col)).cast(T.StringType()))\n",
    "        .withColumn(original_title_col, when(col(original_title_col).isNull(), col(primary_title_col)).otherwise(col(original_title_col)).cast(T.StringType()))\n",
    "        .withColumn(primary_title_col, ud(col(primary_title_col)))\n",
    "        .withColumn(original_title_col, ud(col(original_title_col)))\n",
    "        .withColumn(end_year_col, when(col(end_year_col).isNull() | (col(end_year_col) == \"\\\\N\"), col(start_year_col)).otherwise(col(end_year_col)).cast(T.IntegerType()))\n",
    "        .withColumn(start_year_col, when(col(start_year_col).isNull() | (col(start_year_col) == \"\\\\N\"), col(end_year_col)).otherwise(col(start_year_col)).cast(T.IntegerType()))\n",
    "        .withColumn(runtime_minutes_col, when(col(runtime_minutes_col).isNull(), None).otherwise(col(runtime_minutes_col)).cast(T.IntegerType()))\n",
    "        .withColumn(runtime_minutes_col, when(col(runtime_minutes_col).isNull(), 0).otherwise(col(runtime_minutes_col)).cast(T.IntegerType()))\n",
    "        .withColumn(num_votes_col, when(col(num_votes_col).isNull(), 0).otherwise(col(num_votes_col)).cast(T.IntegerType()))\n",
    "        .withColumn('popularityScore', (col(num_votes_col) / (2022 - col(start_year_col))).cast(T.IntegerType()))\n",
    "        .withColumn('hasSequel', count(\"*\").over(Window.partitionBy(primary_title_col)) > 1)\n",
    "    )\n",
    "    \n",
    "    # Convert label column to integer type if specified\n",
    "    if label_col is not None:\n",
    "        cleaned_df = cleaned_df.withColumn(label_col, col(label_col).cast(T.IntegerType()))\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0c74e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transformers_pipeline(primary_title_col,start_year_col,runtime_minutes_col, num_votes_col,directors_col, movies_by_director_col, avg_votes_director_col, num_writers_col, total_movies_written_col, num_votes_is_null_col, runtime_minutes_is_null_col, popularity_score_col, has_sequel_col):\n",
    "    \n",
    "    PrTitleTok = Tokenizer(inputCol=primary_title_col, outputCol=\"TokenizedPrimaryTitle\")\n",
    "    PrTitleTokStop = StopWordsRemover(inputCol=\"TokenizedPrimaryTitle\", outputCol=\"TokenizedStoppedPrimaryTitle\")\n",
    "    PrTitleTokStopCount = CountVectorizer(inputCol=\"TokenizedStoppedPrimaryTitle\", outputCol=\"TokenizedStoppedCountedPrimaryTitle\")\n",
    "    SYDiscret = QuantileDiscretizer(numBuckets=20, inputCol=start_year_col, outputCol=\"DiscretStartYear\")\n",
    "    RTMDiscret = QuantileDiscretizer(numBuckets=3, inputCol=runtime_minutes_col, outputCol=\"DiscretRTM\")\n",
    "    NVDiscret = QuantileDiscretizer(numBuckets=4, inputCol=num_votes_col, outputCol=\"DiscretNV\")\n",
    "    directorsIndexer = StringIndexer(inputCol=directors_col, outputCol=\"IndexedDirectors\", handleInvalid=\"keep\")\n",
    "    directorsEncoder = OneHotEncoder(inputCol=\"IndexedDirectors\", outputCol=\"EncodedDirectors\")\n",
    "    \n",
    "    # Additional transformers\n",
    "    MBD_Discret = QuantileDiscretizer(numBuckets=4, inputCol=movies_by_director_col, outputCol=\"DiscretMBD\")\n",
    "    AVD_Discret = QuantileDiscretizer(numBuckets=4, inputCol=avg_votes_director_col, outputCol=\"DiscretAVD\")\n",
    "    NW_Discret = QuantileDiscretizer(numBuckets=4, inputCol=num_writers_col, outputCol=\"DiscretNW\")\n",
    "    TMW_Discret = QuantileDiscretizer(numBuckets=4, inputCol=total_movies_written_col, outputCol=\"DiscretTMW\")\n",
    "    \n",
    "    all_assembled = VectorAssembler(\n",
    "        inputCols=[\n",
    "            \"TokenizedStoppedCountedPrimaryTitle\", \n",
    "            \"DiscretRTM\", \n",
    "            \"DiscretNV\", \n",
    "            \"DiscretStartYear\", \n",
    "            runtime_minutes_col, \n",
    "            num_votes_col, \n",
    "            start_year_col,\n",
    "            popularity_score_col,\n",
    "            \"EncodedDirectors\", \n",
    "            MBD_Discret.getOutputCol(), \n",
    "            AVD_Discret.getOutputCol(), \n",
    "            NW_Discret.getOutputCol(), \n",
    "            TMW_Discret.getOutputCol(),\n",
    "            movies_by_director_col, \n",
    "            avg_votes_director_col, \n",
    "            num_writers_col, \n",
    "            total_movies_written_col,\n",
    "            num_votes_is_null_col,\n",
    "            runtime_minutes_is_null_col,\n",
    "            has_sequel_col\n",
    "        ], \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        PrTitleTok,\n",
    "        PrTitleTokStop,\n",
    "        PrTitleTokStopCount,\n",
    "        SYDiscret,\n",
    "        RTMDiscret,\n",
    "        NVDiscret,\n",
    "        directorsIndexer,\n",
    "        directorsEncoder,\n",
    "        MBD_Discret,\n",
    "        AVD_Discret,\n",
    "        NW_Discret,\n",
    "        TMW_Discret,\n",
    "        all_assembled,\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1dd6a8c",
   "metadata": {},
   "source": [
    "### Model training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14f5c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_df_to_train, label_col, feature_col, preprocessing_stages, split_ratio, seed):\n",
    "    classifier = GBTClassifier(labelCol=label_col, featuresCol=feature_col, seed=seed)\n",
    "    pipeline = Pipeline(stages=preprocessing_stages + [classifier])\n",
    "    df_train, df_test = input_df_to_train.randomSplit([split_ratio, 1.0 - split_ratio], seed=seed)\n",
    "    model_trained = pipeline.fit(df_train)\n",
    "\n",
    "    return model_trained, df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28677af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_trained, test_data, label_col=None):\n",
    "    predictions = model_trained.transform(test_data)\n",
    "    evaluator_multi = MulticlassClassificationEvaluator(labelCol=label_col)\n",
    "    evaluator_binary = BinaryClassificationEvaluator(labelCol=label_col)\n",
    "    accuracy_multi = round(evaluator_multi.evaluate(predictions, {evaluator_multi.metricName: \"accuracy\"}), 3)\n",
    "    accuracy_binary = round(evaluator_binary.evaluate(predictions, {evaluator_binary.metricName: \"areaUnderROC\"}), 3)\n",
    "    print(f\"Accuracy multiclass: {accuracy_multi}\")\n",
    "    print(f\"Accuracy binary: {accuracy_binary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06a4976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_final_model(model_trained, input_path, file_prefix, schema_csv, primary_title_col, original_title_col, start_year_col, end_year_col, runtime_minutes_col, num_votes_col, output_file_name, director_data_file, writer_data_file):\n",
    "\n",
    "    # Read hidden file to DataFrame\n",
    "    final_df = read_training_files_to_dataframe(directory=input_path, file_prefix=file_prefix, schema_csv=schema_csv)\n",
    "\n",
    "    # Clean DataFrame columns\n",
    "    final_df_cleaned = clean_dataframe_columns(final_df, primary_title_col, original_title_col, start_year_col, end_year_col, runtime_minutes_col, num_votes_col)\n",
    "\n",
    "    # Load director and writer data\n",
    "    df_directors = load_director_data(director_data_file)\n",
    "    df_writers = load_writer_data(writer_data_file)\n",
    "\n",
    "    # Add director and writer data\n",
    "    df_cleaned_with_director_data = add_director_data(final_df_cleaned, df_directors)\n",
    "    df_cleaned_with_director_and_writer_data = add_writer_data(df_cleaned_with_director_data, df_writers)\n",
    "\n",
    "    # Transform final model\n",
    "    final_model_transformed = model_trained.transform(df_cleaned_with_director_and_writer_data)\n",
    "\n",
    "    # Convert transformed DataFrame to pandas DataFrame\n",
    "    final_model_transformed_pandas = final_model_transformed.select('prediction').toPandas().astype(bool)\n",
    "\n",
    "    # Save pandas DataFrame to CSV file\n",
    "    final_model_transformed_pandas.to_csv(output_file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "632f2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model_trained):\n",
    "    # Extract the feature importances and feature names\n",
    "    pipeline_stages = model_trained.stages\n",
    "    vector_assembler_stage = [stage for stage in pipeline_stages if isinstance(stage, VectorAssembler)][0]\n",
    "    feature_importances = model_trained.stages[-1].featureImportances.toArray()\n",
    "    feature_names = vector_assembler_stage.getInputCols()\n",
    "    # Create a list of (feature_name, feature_importance) tuples\n",
    "    feature_tuples = list(zip(feature_names, [float(val) for val in feature_importances]))\n",
    "    # Create a DataFrame with the feature importances\n",
    "    schema = StructType([\n",
    "        StructField(\"feature_name\", StringType(), True),\n",
    "        StructField(\"feature_importance\", DoubleType(), True)])\n",
    "    feature_df = spark.createDataFrame(feature_tuples, schema)\n",
    "    return feature_df.orderBy(desc(\"feature_importance\")).show()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a129bd",
   "metadata": {},
   "source": [
    "### Data preparation parameters and execute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "14fa0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory and file_pre_fix\n",
    "directory = \".\"\n",
    "file_prefix = \"train\"\n",
    "\n",
    "# Schema parameters (train data sets include 'label' column)\n",
    "schema_csv = StructType([\n",
    "  StructField(\"\", IntegerType(), nullable = True),\n",
    "  StructField(\"tconst\", StringType(), nullable = True),\n",
    "  StructField(\"primaryTitle\", StringType(), nullable = True),\n",
    "  StructField(\"originalTitle\", StringType(), nullable = True),\n",
    "  StructField(\"startYear\", IntegerType(), nullable = True),\n",
    "  StructField(\"endYear\", IntegerType(), nullable = True),\n",
    "  StructField(\"runtimeMinutes\", IntegerType(), nullable = True),\n",
    "  StructField(\"numVotes\", DoubleType(), nullable = True),\n",
    "  StructField(\"label\", BooleanType(), nullable = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26d1679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for multiple functions\n",
    "\n",
    "primary_title_col = \"primaryTitle\"\n",
    "original_title_col = 'originalTitle'\n",
    "start_year_col = \"startYear\"\n",
    "end_year_col = \"endYear\"\n",
    "runtime_minutes_col = \"runtimeMinutes\"\n",
    "num_votes_col = \"numVotes\"\n",
    "directors_col = \"directors\"\n",
    "label = 'label'\n",
    "movies_by_director_col = \"moviesByDirector\"\n",
    "avg_votes_director_col = \"avgVotesDirector\"\n",
    "num_writers_col = \"numWriters\"\n",
    "total_movies_written_col = \"totalMoviesWritten\"\n",
    "num_votes_is_null_col = \"numVotesIsNull\" \n",
    "runtime_minutes_is_null_col = \"runtimeMinutesIsNull\"\n",
    "popularity_score_col = \"popularityScore\"\n",
    "has_sequel_col = \"hasSequel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9fbbcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark session and read data\n",
    "findspark.init()\n",
    "spark = create_spark_session()\n",
    "df = read_training_files_to_dataframe(directory=directory, file_prefix=file_prefix, schema_csv=schema_csv)\n",
    "\n",
    "# Clean data\n",
    "df_cleaned = clean_dataframe_columns(df, primary_title_col, original_title_col, start_year_col, end_year_col, runtime_minutes_col, num_votes_col, label)\n",
    "\n",
    "# Get global transformers\n",
    "preprocessing_stages = feature_transformers_pipeline(primary_title_col, start_year_col, runtime_minutes_col, num_votes_col, directors_col, movies_by_director_col, avg_votes_director_col, num_writers_col, total_movies_written_col, num_votes_is_null_col, runtime_minutes_is_null_col, popularity_score_col, has_sequel_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d8c0e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Add director and writer data\n",
    "\n",
    "director_data_file = '/Users/xandersnelder/repos/big-data-project/directing.json'\n",
    "writer_data_file = '/Users/xandersnelder/repos/big-data-project/writing.json'\n",
    "\n",
    "directors = load_director_data(director_data_file)\n",
    "writers = load_writer_data(writer_data_file)\n",
    "\n",
    "df_cleaned_with_director_data = add_director_data(df_cleaned, directors)\n",
    "df_cleaned_with_director_and_writer_data = add_writer_data(df_cleaned_with_director_data, writers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "966e51bc",
   "metadata": {},
   "source": [
    "### Model training parameters and execute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0941ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling parameters\n",
    "input_df = df_cleaned_with_director_and_writer_data\n",
    "feature_col = 'features'\n",
    "label_col = 'label'\n",
    "split_ratio = 0.7\n",
    "seed = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f78a8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:19:51 WARN DAGScheduler: Broadcasting large task binary with size 1234.7 KiB\n",
      "23/03/20 22:19:51 WARN DAGScheduler: Broadcasting large task binary with size 1234.8 KiB\n",
      "23/03/20 22:19:51 WARN DAGScheduler: Broadcasting large task binary with size 1301.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:19:53 WARN DAGScheduler: Broadcasting large task binary with size 1514.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:19:54 WARN DAGScheduler: Broadcasting large task binary with size 1515.6 KiB\n",
      "23/03/20 22:19:54 WARN DAGScheduler: Broadcasting large task binary with size 1516.2 KiB\n",
      "23/03/20 22:19:54 WARN DAGScheduler: Broadcasting large task binary with size 1517.8 KiB\n",
      "23/03/20 22:19:55 WARN DAGScheduler: Broadcasting large task binary with size 1520.5 KiB\n",
      "23/03/20 22:19:55 WARN DAGScheduler: Broadcasting large task binary with size 1526.7 KiB\n",
      "23/03/20 22:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1527.4 KiB\n",
      "23/03/20 22:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1527.9 KiB\n",
      "23/03/20 22:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1529.1 KiB\n",
      "23/03/20 22:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1531.4 KiB\n",
      "23/03/20 22:19:57 WARN DAGScheduler: Broadcasting large task binary with size 1534.1 KiB\n",
      "23/03/20 22:19:57 WARN DAGScheduler: Broadcasting large task binary with size 1534.8 KiB\n",
      "23/03/20 22:19:57 WARN DAGScheduler: Broadcasting large task binary with size 1535.3 KiB\n",
      "23/03/20 22:19:58 WARN DAGScheduler: Broadcasting large task binary with size 1536.5 KiB\n",
      "23/03/20 22:19:58 WARN DAGScheduler: Broadcasting large task binary with size 1539.0 KiB\n",
      "23/03/20 22:19:58 WARN DAGScheduler: Broadcasting large task binary with size 1541.6 KiB\n",
      "23/03/20 22:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1542.3 KiB\n",
      "23/03/20 22:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1542.9 KiB\n",
      "23/03/20 22:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1544.0 KiB\n",
      "23/03/20 22:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1546.5 KiB\n",
      "23/03/20 22:20:00 WARN DAGScheduler: Broadcasting large task binary with size 1549.6 KiB\n",
      "23/03/20 22:20:00 WARN DAGScheduler: Broadcasting large task binary with size 1550.3 KiB\n",
      "23/03/20 22:20:00 WARN DAGScheduler: Broadcasting large task binary with size 1550.9 KiB\n",
      "23/03/20 22:20:00 WARN DAGScheduler: Broadcasting large task binary with size 1552.1 KiB\n",
      "23/03/20 22:20:00 WARN DAGScheduler: Broadcasting large task binary with size 1554.4 KiB\n",
      "23/03/20 22:20:01 WARN DAGScheduler: Broadcasting large task binary with size 1557.0 KiB\n",
      "23/03/20 22:20:01 WARN DAGScheduler: Broadcasting large task binary with size 1557.5 KiB\n",
      "23/03/20 22:20:01 WARN DAGScheduler: Broadcasting large task binary with size 1558.1 KiB\n",
      "23/03/20 22:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1559.5 KiB\n",
      "23/03/20 22:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1561.7 KiB\n",
      "23/03/20 22:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1564.3 KiB\n",
      "23/03/20 22:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1564.8 KiB\n",
      "23/03/20 22:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1565.6 KiB\n",
      "23/03/20 22:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1567.0 KiB\n",
      "23/03/20 22:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1569.4 KiB\n",
      "23/03/20 22:20:04 WARN DAGScheduler: Broadcasting large task binary with size 1572.0 KiB\n",
      "23/03/20 22:20:04 WARN DAGScheduler: Broadcasting large task binary with size 1572.5 KiB\n",
      "23/03/20 22:20:04 WARN DAGScheduler: Broadcasting large task binary with size 1573.1 KiB\n",
      "23/03/20 22:20:04 WARN DAGScheduler: Broadcasting large task binary with size 1574.3 KiB\n",
      "23/03/20 22:20:05 WARN DAGScheduler: Broadcasting large task binary with size 1576.3 KiB\n",
      "23/03/20 22:20:05 WARN DAGScheduler: Broadcasting large task binary with size 1578.0 KiB\n",
      "23/03/20 22:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1578.4 KiB\n",
      "23/03/20 22:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1579.0 KiB\n",
      "23/03/20 22:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1580.2 KiB\n",
      "23/03/20 22:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1582.8 KiB\n",
      "23/03/20 22:20:07 WARN DAGScheduler: Broadcasting large task binary with size 1585.4 KiB\n",
      "23/03/20 22:20:07 WARN DAGScheduler: Broadcasting large task binary with size 1585.8 KiB\n",
      "23/03/20 22:20:07 WARN DAGScheduler: Broadcasting large task binary with size 1586.6 KiB\n",
      "23/03/20 22:20:08 WARN DAGScheduler: Broadcasting large task binary with size 1588.2 KiB\n",
      "23/03/20 22:20:08 WARN DAGScheduler: Broadcasting large task binary with size 1590.3 KiB\n",
      "23/03/20 22:20:08 WARN DAGScheduler: Broadcasting large task binary with size 1592.4 KiB\n",
      "23/03/20 22:20:09 WARN DAGScheduler: Broadcasting large task binary with size 1592.9 KiB\n",
      "23/03/20 22:20:09 WARN DAGScheduler: Broadcasting large task binary with size 1593.7 KiB\n",
      "23/03/20 22:20:09 WARN DAGScheduler: Broadcasting large task binary with size 1594.8 KiB\n",
      "23/03/20 22:20:09 WARN DAGScheduler: Broadcasting large task binary with size 1596.8 KiB\n",
      "23/03/20 22:20:10 WARN DAGScheduler: Broadcasting large task binary with size 1598.8 KiB\n",
      "23/03/20 22:20:10 WARN DAGScheduler: Broadcasting large task binary with size 1599.3 KiB\n",
      "23/03/20 22:20:10 WARN DAGScheduler: Broadcasting large task binary with size 1599.8 KiB\n",
      "23/03/20 22:20:10 WARN DAGScheduler: Broadcasting large task binary with size 1601.1 KiB\n",
      "23/03/20 22:20:11 WARN DAGScheduler: Broadcasting large task binary with size 1602.8 KiB\n",
      "23/03/20 22:20:11 WARN DAGScheduler: Broadcasting large task binary with size 1605.0 KiB\n",
      "23/03/20 22:20:11 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "23/03/20 22:20:11 WARN DAGScheduler: Broadcasting large task binary with size 1606.1 KiB\n",
      "23/03/20 22:20:12 WARN DAGScheduler: Broadcasting large task binary with size 1607.0 KiB\n",
      "23/03/20 22:20:12 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "23/03/20 22:20:12 WARN DAGScheduler: Broadcasting large task binary with size 1610.4 KiB\n",
      "23/03/20 22:20:13 WARN DAGScheduler: Broadcasting large task binary with size 1610.9 KiB\n",
      "23/03/20 22:20:13 WARN DAGScheduler: Broadcasting large task binary with size 1611.6 KiB\n",
      "23/03/20 22:20:13 WARN DAGScheduler: Broadcasting large task binary with size 1612.9 KiB\n",
      "23/03/20 22:20:13 WARN DAGScheduler: Broadcasting large task binary with size 1615.1 KiB\n",
      "23/03/20 22:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1617.0 KiB\n",
      "23/03/20 22:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1617.4 KiB\n",
      "23/03/20 22:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1618.0 KiB\n",
      "23/03/20 22:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1618.9 KiB\n",
      "23/03/20 22:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1620.3 KiB\n",
      "23/03/20 22:20:15 WARN DAGScheduler: Broadcasting large task binary with size 1622.1 KiB\n",
      "23/03/20 22:20:15 WARN DAGScheduler: Broadcasting large task binary with size 1622.6 KiB\n",
      "23/03/20 22:20:15 WARN DAGScheduler: Broadcasting large task binary with size 1623.2 KiB\n",
      "23/03/20 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 1624.0 KiB\n",
      "23/03/20 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 1625.0 KiB\n",
      "23/03/20 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 1626.4 KiB\n",
      "23/03/20 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 1626.9 KiB\n",
      "23/03/20 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 1627.5 KiB\n",
      "23/03/20 22:20:17 WARN DAGScheduler: Broadcasting large task binary with size 1628.7 KiB\n",
      "23/03/20 22:20:17 WARN DAGScheduler: Broadcasting large task binary with size 1630.3 KiB\n",
      "23/03/20 22:20:17 WARN DAGScheduler: Broadcasting large task binary with size 1631.9 KiB\n",
      "23/03/20 22:20:18 WARN DAGScheduler: Broadcasting large task binary with size 1632.4 KiB\n",
      "23/03/20 22:20:18 WARN DAGScheduler: Broadcasting large task binary with size 1633.0 KiB\n",
      "23/03/20 22:20:18 WARN DAGScheduler: Broadcasting large task binary with size 1633.6 KiB\n",
      "23/03/20 22:20:18 WARN DAGScheduler: Broadcasting large task binary with size 1634.2 KiB\n",
      "23/03/20 22:20:18 WARN DAGScheduler: Broadcasting large task binary with size 1635.9 KiB\n",
      "23/03/20 22:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1636.5 KiB\n",
      "23/03/20 22:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1637.1 KiB\n",
      "23/03/20 22:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1637.9 KiB\n",
      "23/03/20 22:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1638.8 KiB\n",
      "23/03/20 22:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1640.6 KiB\n",
      "23/03/20 22:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1641.1 KiB\n",
      "23/03/20 22:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1641.7 KiB\n",
      "23/03/20 22:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1642.8 KiB\n",
      "23/03/20 22:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1643.9 KiB\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_trained, df_train, df_test = train_model(input_df_to_train=input_df, label_col=label_col, feature_col=feature_col, preprocessing_stages=preprocessing_stages, split_ratio=split_ratio, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9f05d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:22:00 WARN DAGScheduler: Broadcasting large task binary with size 1382.1 KiB\n",
      "23/03/20 22:22:01 WARN DAGScheduler: Broadcasting large task binary with size 1370.5 KiB\n",
      "Accuracy multiclass: 0.74\n",
      "Accuracy binary: 0.811\n"
     ]
    }
   ],
   "source": [
    "### After\n",
    "# Evaluate model\n",
    "evaluate_model(model_trained, df_test, label_col=label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 18:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1350.0 KiB\n",
      "23/03/20 18:15:19 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/03/20 18:15:19 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/03/20 18:15:19 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/03/20 18:15:20 WARN DAGScheduler: Broadcasting large task binary with size 1338.8 KiB\n",
      "Accuracy multiclass: 0.737\n",
      "Accuracy binary: 0.814\n"
     ]
    }
   ],
   "source": [
    "### Before\n",
    "# Evaluate model\n",
    "evaluate_model(model_trained, df_test, label_col=label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9f9e9360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        feature_name|  feature_importance|\n",
      "+--------------------+--------------------+\n",
      "|            numVotes| 0.02393080160316427|\n",
      "|    DiscretStartYear|0.015831260553492342|\n",
      "|     popularityScore|0.008974741312843468|\n",
      "|    EncodedDirectors|0.003380304803443...|\n",
      "|          DiscretMBD|0.002060074607553...|\n",
      "|           DiscretNV|0.002049926864719...|\n",
      "|           hasSequel|0.002007664378818677|\n",
      "|      numVotesIsNull|0.001904880495982...|\n",
      "|           DiscretNW|0.001633585676360...|\n",
      "|           startYear| 4.14911793739783E-5|\n",
      "|TokenizedStoppedC...|4.117712910756262E-6|\n",
      "|  totalMoviesWritten| 8.37076221972239E-8|\n",
      "|          DiscretRTM|                 0.0|\n",
      "|          DiscretTMW|                 0.0|\n",
      "|      runtimeMinutes|                 0.0|\n",
      "|    moviesByDirector|                 0.0|\n",
      "|runtimeMinutesIsNull|                 0.0|\n",
      "|          DiscretAVD|                 0.0|\n",
      "|    avgVotesDirector|                 0.0|\n",
      "|          numWriters|                 0.0|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect feature importances\n",
    "feature_importances = get_feature_importances(model_trained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "967a20dd",
   "metadata": {},
   "source": [
    "### Final model parameters and export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cbc61b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema parameters (hidden files do not have 'label' column)\n",
    "final_schema_csv = StructType([\n",
    "  StructField(\"\", IntegerType(), nullable = True),\n",
    "  StructField(\"tconst\", StringType(), nullable = True),\n",
    "  StructField(\"primaryTitle\", StringType(), nullable = True),\n",
    "  StructField(\"originalTitle\", StringType(), nullable = True),\n",
    "  StructField(\"startYear\", IntegerType(), nullable = True),\n",
    "  StructField(\"endYear\", IntegerType(), nullable = True),\n",
    "  StructField(\"runtimeMinutes\", IntegerType(), nullable = True),\n",
    "  StructField(\"numVotes\", DoubleType(), nullable = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00619ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input directory\n",
    "input_path = '/Users/xandersnelder/repos/big-data-project/'\n",
    "\n",
    "# Validation hidden parameters\n",
    "validation_hidden = 'validation_hidden'\n",
    "hidden_output_file_name = 'output_validation_hidden.txt'\n",
    "\n",
    "# Test hidden parameters\n",
    "test_hidden = 'test_hidden'\n",
    "output_test_hidden = 'output_test_hidden.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33b3de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "directors = load_director_data(director_data_file)\n",
    "writers = load_writer_data(writer_data_file)\n",
    "\n",
    "df_cleaned_with_director_data = add_director_data(df_cleaned, directors)\n",
    "df_cleaned_with_director_and_writer_data = add_writer_data(df_cleaned_with_director_data, writers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d11538d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:20:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Train and export final model for validation_hidden\n",
    "export_final_model(\n",
    "    model_trained=model_trained,\n",
    "    input_path=input_path, \n",
    "    file_prefix=validation_hidden, \n",
    "    schema_csv=final_schema_csv, \n",
    "    primary_title_col=primary_title_col, \n",
    "    original_title_col=original_title_col, \n",
    "    start_year_col=start_year_col, \n",
    "    end_year_col=end_year_col, \n",
    "    runtime_minutes_col=runtime_minutes_col, \n",
    "    num_votes_col=num_votes_col, \n",
    "    output_file_name = hidden_output_file_name,\n",
    "    director_data_file = director_data_file, \n",
    "    writer_data_file = writer_data_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08055fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/xandersnelder/opt/anaconda3/envs/mypython3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 22:20:28 WARN DAGScheduler: Broadcasting large task binary with size 1357.4 KiB\n"
     ]
    }
   ],
   "source": [
    "# Train and export final model for test_hidden\n",
    "export_final_model(\n",
    "    model_trained=model_trained,\n",
    "    input_path=input_path, \n",
    "    file_prefix=test_hidden, \n",
    "    schema_csv=final_schema_csv, \n",
    "    primary_title_col=primary_title_col, \n",
    "    original_title_col=original_title_col, \n",
    "    start_year_col=start_year_col, \n",
    "    end_year_col=end_year_col, \n",
    "    runtime_minutes_col=runtime_minutes_col, \n",
    "    num_votes_col=num_votes_col, \n",
    "    output_file_name = output_test_hidden,\n",
    "    director_data_file = director_data_file, \n",
    "    writer_data_file = writer_data_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efef71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
